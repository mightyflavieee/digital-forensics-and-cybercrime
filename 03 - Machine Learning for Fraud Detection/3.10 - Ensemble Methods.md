# 3.10 Ensemble Methods

## 3.10.1 Intro
üí° #IDEA **Aim at estimating multiple analytical models instead of using only one.**

**Multiple models** can:
- *Cover different parts of the data* input space and as such complement each other‚Äôs deficiencies.
- Be *sensitive to changes* in the underlying data.
Commonly *used with decision trees*:
- [[#3 10 2 Bagging Bootstrap aggregating Bagging | Bagging.]]
- [[#3 10 3 Boosting Boosting | Boosting.]]
- [[#3 10 4 Random Forests RandomForests | Random forests.]]

## 3.10.2 Bagging (Bootstrap aggregating) #Bagging

1. Starts by *taking B bootstraps from the underlying sample*. A bootstrap is a sample with replacement.
2. Build a classifier for every bootstrap:
   - For **classification**, a new observation will be *classified by letting all B classifiers vote*.
   - For **regression**, the prediction is the *average of the outcome of the B models*.
   
<u>The number of bootstraps B can either be fixed (e.g., 30) or tuned via an independent validation data set.</u>

### 3.10.2.1 Instability
üîë **Key element for bagging**: *instability of the analytical technique*.

> If perturbing the data set by means of the bootstrapping procedure can alter the model constructed, then bagging will improve the accuracy

<u>For models that are robust with respect to the underlying data set, Bagging will not give much added value.</u>

## 3.10.3 Boosting #Boosting
Estimate multiple models **using a weighted data sample**.

1.  Starting from *uniform weights*.
2.  Iteratively *re-weight the data according to the classification error*:
	- <u>Misclassified cases get higher weights</u>.  

üí° #IDEA: **Difficult observations should get more attention.**

> The final ensemble model is then a *weighted combination* of all the individual models.

A *popular implementation* of this is the Adaptive boosting/*Adaboost* procedure.

<u>The number of boosting runs can be fixed or tuned using an independent validation set.</u>

üîë **Key advantage**: *Easy to implement*.

‚§¥Ô∏è **Potential drawback**: *Risk of overfitting* to the hard (potentially noisy) examples in the data, which will get higher weights as the algorithm proceeds. This is *especially relevant in a fraud detection* setting because the <u>target labels are typically quite noisy</u>.


## 3.10.4 Random Forests #RandomForests
It creates a *forest of decision trees*:

![[random_forests_intro.png|500]]

> **Random forests** can be *used with both classification trees and regression trees*.

üîë **Key concepts**: 
- The *dissimilarity amongst the base classifiers*, which is obtained by adopting a <u> bootstrapping procedure to select the training samples of the individual base classifiers</u>.
- The *selection of a random subset of attributes at each node*.
- The *strength of the individual base models*.

<u>The **diversity of the base classifiers** creates an ensemble that is superior in performance compared to the single models.</u>


## 3.10.4 Evaluating Ensemble Methods
<u>Various benchmarking studies</u> have shown that *random forests can achieve excellent predictive performance*:
- They rank amongst the **best performing models** across a wide variety of prediction tasks .
- They can *deal with data sets having only a few observations*, but **with lots of variables**.
- They are **highly recommended when high performing analytical methods are needed for fraud detection**.
- They are a *black-box models*.
	- <u>Due to the multitude of decision trees</u> that make up the ensemble, it is very hard to see how the final classification is made.

### 3.10.4.1 Variable Importance #VI
One way to *shed some light on the internal workings of an ensemble* is by **calculating the variable importance (VI)**.

A <u>popular procedure</u> to do so is as follows:
1.  *Permute the values of the variable* under consideration on the <u>validation or test set</u>.
2.  *For each tree, calculate the VI value*, the **difference between the error on the original, unpermuted data, and the error on the permuted data**.
	- In a *regression setting, the error can be the MSE*, whereas in a <u>classification setting, the error can be the misclassification rate</u>.
3. *Order all variables according to their VI value*. **The variable with the highest VI value is the most important**.

---

Next chapter: [[3.11 - Evaluating a Fraud Detection Model | Evaluating a Fraud Detection Model]]