# 3.4 Clustering #Clustering
## 3.4.1 Intro
With **Clusterig**, a set of **observations** (dataset) is **split** into different **segments** that have the following properties:
- **Homogeneity** <u>within</u> a **segment** is **maximized**;
- **Heterogeneity** <u>between</u> **segments** is **maximized**.

The data can be:
- Structured
- Unstructured

> **Avoid** excessive amounts of **correlated data** by applying feature selection methods.

## 3.4.2 Clustering for Fraud Detection 
![[clustering_for_fraud.png]]
When using clustering for fraud detection, these techniques can be identified:
- [[#3 4 3 Hierarchical Techniques HierarchicalTechniques | Hierarchical]]:
	1. [[#^6eaab7 | Agglomerative]];
	2. [[#^f45c50 | Divisive]].
- [[#3 4 4 Non-Hierarchical Techniques Non-HierarchicalTechniques | Non-hierarchical]]:
	1. [[#^91668b | k-means]];
	2. [[#^467f70 | Self Organizing Map (SOM)]].

### Distance Metrics #DistanceMetrics
Since the aim of clustering is grouping observations based on similarity, a distance metric is needed to quantify this similarity.

Simple distance metrics:
- <u>Euclidean Distance</u>
- <u>Minkowski distance</u>
	**Formula:** $D(x_i; x_j) = (\sum^n_{k=1} |x_{ik} - x_{jk}|^{p})^{\frac{1}{p}}$ 
	if p=1 ==> Manhattan or City block distance
	if p=2 ==> Euclidean distance

Techniques based on types of variables:
- **Continuous Variables:** 
	1. Euclidean metric;
	2. Pearson correlation;
	3. Cosine measure.
- **Categorical variables:** With binary variables two techniques:
	1. <u>Simple Matching Coefficient (SMC)</u> = calculates the number of identical matches between the variable values (equally important).
	2. <u>Jaccard index</u> = measures the similarity between both claims across those red flags that were raised at least once.
- **Continuous and Categorical Mix:** complicates the computation of the distance. Two options:
	1. Code the categorical variables as 0/1 dummies and then use a Continuous distance measure;
	2. Use a weighted combination of distance measures, which is less straightforward less frequently used

## 3.4.3 Hierarchical Techniques #HierarchicalTechniques
Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. Two types can be identified:
- #Agglomerative **Agglomerative**: <u>Bottom-Up</u> approach, each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy; ^6eaab7
- #Divisive **Divisive** :  <u>Top-Down</u> approach, all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. ^f45c50

### Linkeage #Linkeage
Distance between clusters can be measured in different ways, such as:
- #SingleLinkeage **Single linkeage:** <u>Minimum distance</u> between two points in the two clusters;
- #CompleteLinkeage **Complete linkeage:** <u>Maximum distance</u> between two points in the two clusters;
- #AverageLinkeage **Average linkeage:** <u>Average distance</u> between all the points in the two clusters;
- #CentroidLinkeage **Centroid linkeage:** Distance between the two <u>center points</u> in the two clusters;
- #WardCriterion **Ward's criterion:** $D_{Ward}(C_i; C_j) = \sum_{x \in C_i}(x-c_i)^2 + \sum_{x \in C_j}(x-c_j)^2 - \sum_{x \in C_ij}(x-c_ij)^2$ ;

### Number Of Clusters #NumberOfClusters
To decide on the optimal number of clusters:
- #Dendrogram **Dendrogram:**
	1. Tree-like diagram that records the sequences of merges;
	2. Vertical (or horizontal scale) gives the distance between two clusters amalgamated;
	3. Cut the dendrogram at the desired level to find the optimal clustering.
	![[dendrogram.png]]
	![[dendrogram2.png]]
- #ScreenPlot **Screen plot:** 
	1. Plot of the distance at which clusters are merged;
	2. The elbow point then indicates the optimal clustering.
	![[screen_plot.png]]

### Conclusions
ðŸŸ¢ **Advantages:**
1. The *number of clusters* does *not* need to be *specified prior* to the analysis.

ðŸ”´ **Disadvantages:**
1. *Doesn't scale* well with large datasets;
2. The *interpretation* of the clusters is often *subjective* and depends on the business expert and/or data scientist.

## 3.4.4 Non-Hierarchical Techniques #Non-HierarchicalTechniques 
Non-Hierarchical Clustering can be subdivided in two techniques:
- #K-Means***k*-means**: method that aims to *partition n observations into k clusters* in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid). ^91668b
- #SOM **Self Organizing Map (SOM):**  ^467f70

### 3.4.4.1 K-Means 
K-Means technique consists of four steps:
1. Select ***k* observations** as **initial** cluster **centroids** (seeds);
2. **Assign** each **observation** to the **cluster** that has the **closest centroid** (e.g., Euclidean distance);
3. When **all observations** have been **assigned**, **recalculate** the **positions** of the ***k* centroids** (mean);
4. **Repeat until** the cluster **centroids no** longer **change or** a **fixed number** of iterations is **reached**.

#### Limitations
A k-means approach presents a number of limitations:

ðŸ”´ The number of clusters k needs to be specified before the start of the analysis. The number of clusters can be determined in four ways:
1. Expert based input;
2. Result of another (e.g., hierarchical) clustering procedure
3. Multiple values of *k* are tried out and the resulting clusters evaluated;
4. Try out different seeds to verify the stability of the clustering solution.

ðŸ”´ *k-means* is sensitive to outliers, which are especially relevant in a fraud detection setting. 
**More robust alternatives:** 
1. Use the median (*k-medoid clustering*);
2. For categorical variables, the mode can be used (*k-mode clustering*).

### 3.4.4.2 Self Organizing Map
#DEF **SOM** is a clustering technique that **employs** an **unsupervised learning algorithm** that allows users **to visualize** and **cluster high-dimensional data on** a **low-dimensional grid** of **neurons**.

Two types of grids:
- Rectangular SOM Grid;
- Hexagonal SOM Grid.

![[SOM_graphs.png]]

### Functioning
Each input is connected to all neurons in the output layer with weights $w = [w_1 , ... , w_N]$, $N =$number of variables. All weights are randomly initialized.

When a training vector $x$ is presented, the weight vector of each neuron is compared with $x$ by calculating the distance.
==> The neuron that is most similar to $x$ in Euclidean sense is called the **B**est **M**atching **U**nit (**BMU**).

Euclidean distance: $d(x, w_i) = \sqrt{\sum^N_{i=1}(x_i - w_{ci})^2}$

The weight vector of the **BMU** and its neighbors in the grid are then adapted using the following learning rule:

> $w_i(t+1) = w_i(t) + h_{ci} \cdot [x(t) - w_i(t)]$

where
- $t$ represents the time index during training;
- $h_{ci}(t)$ defines the neighborhood of the BMU $c$.

---
Next chapter: [[3.05 - Semi-supervised clustering | Semi-supervised clustering]]