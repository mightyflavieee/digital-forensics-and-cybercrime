# 3.7 Decision Trees #DecisionTrees
## 3.7.1 Intro
> ðŸ”‘ A decision tree is a **non-parametric supervised learning algorithm**, which is **utilized** for both **classification** and **regression** tasks. It has a **hierarchical, tree structure**, which **consists** of a **root node, branches, internal nodes** and **leaf nodes**.

Three types of nodes can be distinguished:
- #DEF **Root Node:** node without any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes;
- #DEF **Decision Node:** internal nodes;
- #DEF **Leaf Node:** these nodes represent all the possible outcomes within the dataset, they tipically assign the fraud labels.

There are various algorithms to implement the decisions needed to build a decision tree. They are:
- #DEF #SplittingDecision**Splitting decision:** Which variable to split at what value (e.g., Transaction amount is > $100, 000 or not) ^18fd60
- #DEF #StoppingDecision **Stopping decision:** When to stop adding nodes to the tree? ^4b4330
- #DEF #AssignementDecision **Assignment decision:** What class (e.g., fraud or no fraud) to assign to a leaf node? Look at the majority class within the leaf node to make the decision (*winner-take-all learning*). ^eee504

Decision Trees can be divided in:
1. [[#3 7 3 Classification Trees | Classification Trees]]: for categorical variables;
2. [[#3 7 5 Regression Trees | Regression Trees]]: for continuous variables.

## 3.7.2.1 Impurity #Impurity

^c6a25f

To correctly answer a splitting decision, fist the concept of impurity (or chaos) must be defined.

ðŸŽ¯ **Minimal impurity** occurs when all customers are either good or bad.
ðŸŽ¯ **Maximal impurity** occurs when one has the same number of good and bad customers.

![[impurity.png]]

> Decision trees **aim** at **minimizing** the **impurity** in the data.

Two ways to measure impurity:
- **Entropy**
	--> #DEF #Gain ==**Gain** = weighted decrease in entropy==; ^a0b099
- **Gini**.

## 3.7.3  Classification Trees #ClassificationTrees
### 3.7.3.1 Splitting Decision #SplittingDecision 
To answer the [[#^18fd60 | splitting decision]], various candidate splits must be evaluated in terms of their decrease in [[#3 7 2 1 Impurity Impurity | impurity]] .

ðŸŽ¯ A higher [[#^a0b099 | gain]] is preferred.

The decision tree algorithm considers different candidate splits for its (root) node. We can have two strategies:
1. **Greedy and recursive:** Pick the one with the highest gain;
2. **Perfectly parallel:** advantage of increased efficiency.

### 3.7.3.2 Stopping Decision #StoppingDecision 
Within a decision tree we must have a  [[#^4b4330| stopping decision]] , because If the tree continues to split, it will have one leaf node per observation. This phenomena is known as **Overfitting** and implies:
- The tree has become too complex and fails to correctly model or highlight trends in the data;
- It will generalize poorly to new unseen data.

To avoid this, split data into:
1. **Training set:** make splitting decision;
2. **Validation set:** independent sample to monitor the misclassification error (or any other performance metric) as the tree is grown.

<u>Typical split</u>: 70% of the dataset composes the training set while the remaining 30% makes up the validation set.

![[training_set.png]]

The error on the training sample keeps on decreasing as the splits become more and more specific and tailored towards it.

![[validation_set.png]]

On the validation sample, the error will initially decrease, which indicates that the tree splits generalize well.

![[stopping_decision.png]]

At some point the error will increase since the splits become too specific for the training sample as the tree starts to memorize it.

>ðŸ”‘ **Where** the **validation set curve** reaches its **minimum**, the **procedure** should be **stopped**, as **otherwise overfitting** will occur.

## 3.7.4 Regression Trees #RegressionTrees
Decision trees can be used to predict continuous targets.

### 3.7.5.1 Splitting Decision #SplittingDecision 
The impurity, for determining the [[#^18fd60 | splitting decision]], needs to be measured in another way. Two alternatives:
1.  **M**ean **S**quared **E**rror (**MSE**);
2. **Variance** (**ANOVA**) **Test** and **F-statistic**;

#### MSE #MSE
$MSE \space = \frac{1}{n} \cdot \sum^{n}_{i=1}(Y_i - \bar{Y})^2$

where:
- $n$ represents the number of observations in a leaf node;
- $Y_i$ the value of observation $i$ ;
- $Y$ the average of all values in the leaf node;

#### Variance Test #VarianceTest
$F = \frac{\frac{SS_{between}}{B - 1}}{\frac{SS_{whithin}}{n - B}} \space \tilde{} \space \space F_{n-B, B-1}$
$SS_{between} = \sum^{B}_{b=1}n_b \cdot (\bar{Y_b} - \bar{Y})^2$
$SS_{whithin} = \sum^{B}_{b=1}\sum^{n_b}_{i=1}(Y_{bi} - \bar{Y_b})^2$

where:
- B the number of branches of the split;
- $n_b$ the number of observations in branch $b$;
- $Y_b$ the average in branch $b$;
- $Y_bi$ the value of observation $i$ in branch $b$;
- $Y$ the overall average.

#### Wrapping Up
ðŸŽ¯ <u>For the MSE metric:</u> it is **desirable** to have a **low MSE** in a **leaf node** since this indicates that the **node** is more **homogeneous**.

ðŸŽ¯ <u>For the Variance Test metric:</u> **Good splits** favor homogeneity within a node (**low $SS_{within}$**) and heterogeneity between nodes (**high $SS_{between}$**).

### 3.7.5.2 Stop Decision #StoppingDecision 
The [[#^4b4330| stopping decision]] is similar to classification trees but a regression based performance measure is used (e.g., mean squared error, mean absolute deviation, R-squared) on the Y-axis.

### 3.7.5.3 Assignment Decision #AssignementDecision 
The [[#^eee504 | assignment decision]] can be made by assigning the mean (or median) to each leaf node.

## 3.7.5 Decision Trees Properties #DecisionTreesProperties 
Every tree can also be represented as a **rule set**. This means that every path from a root note to a leaf node can be viewed as a simple if-then rule.

##### Example:
![[decision_tree_example.png]]

<u>**Rules:**</u>
1. **If** Transaction amount **>** $100, 000 **And** Unemployed **=** No **Then** <u>No Fraud</u>
2. **If** Transaction amount **>** $100, 000 **And** Unemployed **=** Yes **Then** <u>Fraud</u>
3. **If** Transaction amount **â‰¤** $100, 000 **And** Previous fraud **=** Yes **Then** <u>Fraud</u>
4. **If** Transaction amount **â‰¤** $100, 000 **And** Previous fraud **=** No **Then** <u>No Fraud</u>

## 3.7.6 Decision Trees in Fraud Analytics #DecisionTreesFraudAnalytics
Regarding variables selection:
- Variables that occur at the top of the tree are more predictive;
- Measure the predictive strength of a variable by calculating the **Gain** of a characteristic to gauge its predictive power.

**Advantages:**
ðŸŸ¢ Decision tree gives a white-box model with a clear explanation: interpretable;
ðŸŸ¢ Operationally efficient;
ðŸŸ¢ Powerful techniques and allow for more complex decision boundaries than a logistic regression;
ðŸŸ¢ Nonparametric, no normality or independence assumptions are needed.

**Disadvantages:**
ðŸ”´ Highly dependent on the sample that was used for tree construction. A small variation in the underlying sample might yield a totally different tree.

> ðŸ”‘ An **analytical fraud model** is usefule when used directly into the business environment.

---

Next chapter: [[3.08 - Neural Networks | Neural Networks]]