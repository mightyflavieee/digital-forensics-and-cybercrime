# 3.8 Neural Networks

## 3.8.1 Intro
#DEF **Neural networks are mathematical representations inspired by the functioning of the human brain.**

>Generalizations of existing statistical models.

**Neural networks** can model *very complex patterns and decision boundaries in the data*.

A <u>processing element or neuron</u> performs *two operations*:
1. It **takes the inputs and multiplies them with the weights** (including the intercept term $Î²_0$ , called the bias term)
2. Puts this into a **nonlinear transformation function** (~logistic regression).
	- Logistic (and linear) regression is a neural network with one neuron:
	
![[neuron_neural-networks.png|350]]


## 3.8.2 MultiLayer Perceptron Neural Network #MLP
- **<u>Input Layer</u>**
- **<u>Hidden Layer</u>**
	- Works like a *feature extractor*: c<u>ombine the inputs into features that are then subsequently offered to the output layer</u>.
	- Has a *nonlinear transformation function* f().
- **<u>Output layer</u>**
	- Makes the *prediction*.
	- Has a *linear transformation function*.
	
![[MLP_NN.png|450]]

## 3.8.3 Transformation functions (activation functions)

The activation functions *may differ per neuron*, they are typically **fixed for each layer**:
In the <u>output layer</u>:
- For **classification** (e.g., fraud detection):
	- A *logistic transformation*, since the outputs can then be interpreted as *probabilities*.
- For **regression** (e.g., amount of fraud):
![[activation_functions.png|400]]
In the <u>hidden layer</u>:
- *Hyperbolic tangent* (tanH) activation function.

## 3.8.4 Details

In the *fraud analytics setting, complex patterns rarely occur*.

<u>Recommended</u> to use a *neural networks with one hidden layer*: **universal approximators**, <u>capable of approximating any function to any desired degree of accuracy on a compact interval</u>.

**Data preprocessing**:
- For *continuous variables*, <u>standardization</u> can be used.
- For *categorical variables (only)*, <u>categorization can be used </u>to reduce the number of categories.

## 3.8.5 Weight Learning
The *optimization* (for optimal parameter values) **is more complex**: Iterative algorithm that optimizes a cost-function:
- *Continuous target variable*
	- **Mean Squared Error (MSE) cost function**.
- *Binary target variable*
	- **Maximum Likelihood cost function**.

>The procedure starts from a set of random weights, which are then iteratively adjusted to the patterns in the data using an **optimization algorithm** (e.g., *Back propagation learning, Conjugate gradient*).

ðŸ”‘ **Key issue**: the <u>curvature of the objective function is not convex and may be multimodal</u>.

The **error function** can thus have *multiple local minima* but typically *only one global minimum*.

![[local_global_minima.png|300]]

If the <u>starting weights are chosen in a suboptimal way</u>, *one may get stuck in a local minimum*.

### 3.8.5.1 Preliminary Training
- Try out *different* starting weights.
- Start the *optimization procedure* for a few steps.
- Continue with the *best intermediate solution*.

### 3.8.5.2 Stopping Criterion
The optimization procedure then continues until:
- The *error function shows no further progress*.
- The *weights stop changing substantially*.
- After a *fixed number of optimization steps* (<u>Epochs</u>).

### 3.8.5.3 Hidden Neurons (Weight and) Number
**Number of hidden neurons** -> <u>nonlinearity in data</u>.
- *More complex, nonlinear patterns* -> <u>more hidden neurons</u>.

They should be <u>carefully tuned</u>:
1. Split the data into a *training, validation, and test set*.
2. Vary the *number of hidden neurons*.
3. *Train* a neural network on the training set.
4. *Measure the performance* on the <u>validation set</u>.
5. *Choose* the <u>number of hidden neurons with optimal validation set performance</u>.
6. *Measure the performance* on the <u>independent test set</u>.

### 3.8.5.4 Overfitting Problem
Neural networks can model *very complex patterns* and decision boundaries in the data -> <u>They can even model the noise in the training data</u> -> **Overfitting**

<u>Option 1</u>
- *Training set* -> estimate the weights.
- *Validation set* -> independent data set used to decide when to stop training **and avoid this overfitting**.

<u>Option 2</u>
**Weight regularization**: keep *weights small in absolute sense* to <u>avoid fitting the noise in the data</u>.
- *Add a weight size term* to the objective function of the neural network.

### 3.8.5.5 The Neural Network Black Box
#DEF ***Black-box*: relate inputs to outputs in a mathematically
complex, nontransparent, and opaque way.**

- <u>Applied as high-performance analytical tools in settings where interpretability is not a key concern.</u>
- In application areas where insight into the fraud behavior is important, one needs to <u>be careful with NNs</u>.

Opening the **Neural Network black box**:
1. [[#3 8 5 6 Variable Selection | Variable selection.]]
2. [[#3 8 5 7 Rule Extraction Procedure | Rule extraction.]]
3. [[#3 8 5 8 Two-stage Model Setup | Two-stage models.]]

### 3.8.5.6 Variable Selection
#DEF **Select variables that actively contribute to the NN output.**
In <u>linear and logistic regression</u> -> *inspecting the p-values*.
In <u>neural networks</u> -> *no p-values*.

#### 3.8.5.6.1 Hinton diagram
<u>Visualizes the weights</u> between the inputs and the hidden neurons as squares:
- *Size of the square* is proportional to the <u>**size** of the weight</u>.
- *Color of the square* represents the <u>**sign** of the weight</u> (e.g., black=negative weight and white=positive weight).

![[hinton_diagram.png|400]]

<u>Steps</u>:
1. Inspect the **Hinton diagram** and *remove the variable whose weights are closest to zero*.
2. *Reestimate the neural network with the variable removed*. To speed up the convergence, it could be beneficial to <u>start from the previous weights</u>.
3. *Continue with step 1 until a stopping criterion is met*. The stopping criterion <u>could be a decrease of predictive performance or a fixed number of steps</u>.

#### 3.8.5.6.2 Backward Variable Selection 
Backward Variable Selection is a <u>performance driven selection</u>.

<u>Steps</u>:
1. Build a neural network *with all N variables*.
2. *Remove each variable in turn and reestimate the network*. <u>This will give N networks each having N â€“ 1 variables</u>.
3. *Remove the variable whose absence gives the best performing network* (e.g., in terms of misclassification error, mean squared error).
4. *Repeat* this procedure *until the performance decreases significantly*.

![[backward_variable_selection.png|350]]

#### 3.8.5.6.3 Variable selection - Discussion
**Variable selection**:
- Allows users to *see which variables are important* and which ones are not.
- It *does not offer a clear insight into its internal workings*. <u>The relationship between the inputs and the output remains nonlinear and complex</u>.

> Sampling can be used to make the procedure less resource intensive and more efficient.

### 3.8.5.7 Rule Extraction Procedure
#DEF **Extract if-then classification rules, mimicking the behavior of the neural network:**

#### 3.8.5.7.1 Decompositional technique
#DEF **Decompose the networkâ€™s internal workings by *inspecting weights and/or activation values*.**

<u>Example</u>:

![[decomposition_rule_example.png|500]]

#### 3.8.5.7.2 Pedagogical technique
#DEF **Consider the neural network as a black box and use the neural *network predictions as input* to a white-box analytical technique such as decision trees.**
- Can essentially be used with <u>any underlying algorithm</u>.\

<u>Example</u>:

![[pedagogical_rule_example.png|500]]

### 3.8.5.8 Two-stage Model Setup
1. Estimate an *easy-to-understand model first* (e.g., linear regression, logistic regression).
	- Estimate an <u>easy-to-understand model first</u> (e.g., linear regression, logistic regression).
2. Use a *neural network to predict the errors* <u>made by the simple model</u> using the same set of predictors.
	- <u>Performance benefit</u> of using a nonlinear model.

Both models are then combined in an <u>additive way</u>, for example as follows:

![[two_stage_model_setup.png|450]]

This provides an **ideal balance** *between model interpretability* (which comes from the first part) *and model performance* (which comes from the second part).

<u>Example</u>:

![[two_stage_model_example.png|500]]

---

Next chapter: [[3.09 - Support Vector Machine | Support Vector Machine]]