# 3.6 Predictive Analytics for Fraud Detection

## 3.6.1 Intro
**The aim is to build an analytical model predicting a target measure of interest**

<u>Two types of predictive analytics</u> can be distinguished depending on the measurement level of the target:
- *Regression*.
- *Classification*.

## 3.6.2 Regression #Regression
**<u>Target variable</u>**:
- *Continuous*.
- *Varies* along a predefined interval.
	- Limited (e.g., between 0 and 1).
	- Unlimited (e.g., between 0 and infinity).

## 3.6.3 Classification #Classification
**<u>Target variable</u>**:
- *Categorical*.
- It can only take on a *limited set of predefined values*:
	- **Binary classification**: <u>only two classes</u> are considered (e.g., fraud versus no-fraud).
	- **Multiclass classification**: the target can belong to <u>more than two classes</u> (e.g., severe fraud, medium fraud, no fraud).

## 3.6.4 Target Variable Definition #TargetVariable
**The target fraud indicator is usually hard to (obtain) and determine.**
> - One can *never be fully sure that a certain transaction is fraudulent*.
> - The *target labels* are typically *not noise-free*.

🕵🏼‍♂️ => <u>Complex analytical modeling exercise</u>.

## 3.6.5 Linear Regression #LinearRegression
**Technique to model a continuous target variable.**

The *<u>general formulation of the linear regression model</u>*:
$Y = \beta_0 + \beta_1X_1 + ... + \beta_NX_N$

- *Y* represents the *target variable*.
- $X_1 , ... , X_N$ the *explanatory variables*.
- β *parameters measure the impact on the target variable Y of each of the individual explanatory variables*.

### 3.6.5.1 Parameter Estimation
The β parameters can then be estimated by *minimizing a squared error function*:
$\frac{1}{2}\sum_{i=1}^n e_i^2 = \frac{1}{2}\sum_{i=1}^n (Y_i - \bar{Y_i})^2 = \frac{1}{2}\sum_{i=1}^n (Y_i - (\beta_0 + \beta_1X_{1i} + ... + \beta_NX_{Ni}))^2$

![[MSE.png|500]]

### 3.6.5.2 Ordinary least squares (OLS) regression #OLS
**Minimizing the sum of all error squares.**

![[OLS.png]]

**<u>Goal of Linear regression</u>**: find the *best fit line that can accurately
predict the output* <u>for the continuous dependent variable with the
help of independent variables</u>.

<u>Example</u>:

![[linear_example.png|450]]

<u>Linear regression</u>: $Y = \beta_0 + \beta_1Revenue + \beta_2Employees + \beta_3VATCompliant$

*When estimating this using OLS*, **two key problems arise**:
1. The *errors/target are not normally distributed* but follow a Bernoulli distribution with only two values.
2. There is *no guarantee that the target is between 0 and 1*, which would be handy since it can then be interpreted as a probability.

### 3.6.6 Logistic Regression
Consider now the following **bounding function**:

![[logistic_reg.png|400]]
The *outcome is always between 0 and 1.*

#DEF **Logistic Regression Model**: **Combination of the linear regression with the bounding function**.

 Given $Z = \beta_0 + \beta_1Revenue + \beta_2Employees + \beta_3VATCompliant$
$f(Z)=\frac{1}{1 + e^{-(\beta_0 + \beta_1Revenue + \beta_2Employees + \beta_3VATCompliant)}}$

Outcome: *bounded between 0 and 1* == **probability**.

<u>Then we have</u>: $P(fraud=yes\,|\,Revenue,\, Employees,\,VATCompliant)$

#### 3.6.6.1 Activation Function
We pass the *weighted sum of inputs* through an **activation function** *that can map values in between 0 and 1*.
Such activation function is known as **sigmoid function** and the curve obtained is called as *sigmoid curve or S-curve*.

![[sigmoid.png|400]]

The $β_i$ parameters of a *logistic regression model* are estimated using the **maximum likelihood optimization**.

#### 3.6.6.2 Logistic Regression Property
**It estimates a linear decision boundary to separate both classes.**

![[logistic_property.png|400]]

### 3.6.7 Linear and Logistic Regression

![[logistic_vs_linear.png|500]]

***<u>Linear Regression</u>*** | ***<u>Logistic Regression</u>*** 
:---------------- | ----------------:
Predicting the **continuous** dependent variable with independent variables. | Predict the **categorical** dependent variable with independent variables.
**Predict the output** for the continuous dependent variable. |  It estimates a **linear decision boundary** to separate both classes.
Finds the **linear relationship** between dependent variable and independent variable. | Based on the concept of **Maximum Likelihood estimation**.
Based on **Ordinary Least Squares** | Used for: Classification/Regression/where the probabilities is required.
**Output**: continuous values | **Output**: between the 0 and 1.

---

Next chapter: [[3.07 - Decision Trees | Decision Trees]]